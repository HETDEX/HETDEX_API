{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14 - Extracting a sourcelist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to grab 1D aperture summed HETDEX spectra for an input of ID, RA and DEC using the `Extract` Class API from `HETDEX_API`. This can be done most directly using the command line tool `get_spec.py`. It can be used both interactively on a compute node via `idev` or through a jupyter notebook as shown here, as well as within a slurm job file. We show you here how to do both.\n",
    "\n",
    "Examples of what you might like to do with the spectra afterwards is shown later. For example the spectra produced from get_spec.py can be translated to line intensities, or approximate pass-band magnitudes with a few extra steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import all necessary python packages. \n",
    "These are mainly for working within the notebook. The command line tools described have the necessary preamble built in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# either point to your HETDEX_API path (you will need to git pull) or use Erin's\n",
    "import sys\n",
    "sys.path.append('/work/05350/ecooper/stampede2/HETDEX_API')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from input_utils import setup_logging\n",
    "import astropy.units as u\n",
    "from astropy.io import fits\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy.table import Table, join\n",
    "\n",
    "from hetdex_api.extract import Extract\n",
    "from hetdex_api.survey import Survey\n",
    "from hetdex_api.shot import *\n",
    "\n",
    "import warnings\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTANT NOTE OF CAUTION WITH RUNNING ON TACC!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because this script involves opening the Fibers class object which contains all fiber spectra from a 3 dither observation, you will be pulling in a lot of memory for each shot that is open. **NEVER** run this script from a login node on TACC. A login node is a node you access when you ssh in. \n",
    "\n",
    "You need to request a compute node instead by either \n",
    "\n",
    "(1) using the idev command :\n",
    "\n",
    "`idev -p skx-dev `\n",
    "\n",
    "if it works on a small catalog, you can always run interactively using a longer queue option\n",
    "\n",
    "`idev -p skx-normal` \n",
    "\n",
    "(2) using a jupyter notebook \n",
    "\n",
    "(3) or by submitting the job into the slurm job scheduler (generally if you are working on more than ~200 shots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting all spectra at a specified RA/DEC. This will search through all shots in HDR1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a shotid is not specified the program will search for any shot within HDR1 that overlaps within an 11 arcmin radius of the input coordinates. Because of the non-contiguous VIRUS footprint, there is no guarantee the aperture defined by the input ra/dec/rad will contain enough fibers to do a measurement. The aperture radius is 3\" by default or can be specified with the --rad argument. Use the --outfile argument to label the output pickle file. Otherwise the default 'output.pkl' will be used.\n",
    "\n",
    "You can run these commands from the command line by removing the \"!\" command but be sure you are on a compute node by calling `idev` first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 /work/05350/ecooper/stampede2/HETDEX_API/get_spec.py --ra 150.02548 --dec 2.087987 --ID cosmos_LAE --outfile cosmos_LAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speed things up using multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can speed up processes (by up to ~30x) with python multiprocessing if you are working interactively in a notebook or in an idev session (**NEVER FROM A LOGIN NODE**). Use the multiprocessing option with the argument -mp True or --multiprocess True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 /work/05350/ecooper/stampede2/HETDEX_API/get_spec.py --multiprocess True -ra 150.02548 -dec 2.087987 -id mptest -o mptest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting all spectra at a specified RA/DEC in a specific OBSERVATION/SHOT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps you only want to focus on a specific shot. Then you can use the -s argument to put the shotid either as an interger value 'YYYYMMDDOBS'= 20190104008 or as a str '20190104v009'. Note if you don't give an --ID option the default is 'DEX'\n",
    "\n",
    "This is a command line routine so remove the \"!\" if you are running in a terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 /work/05350/ecooper/stampede2/HETDEX_API/get_spec.py -ra 8.86535 -dec 0.59352  -s 20190104008 -o 20190104008"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is particularly helpful if you plan to submit each shot as a separate task. For this reason, I suggest changing the default --outfile option to -o 20190104008 to create the output pickle file 20190104008"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work on a list of ID/RA/DECs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can either be a saved astropy table, or an space delimited text file with 3 columns where the columns are ID, RA, DEC. If you want more functionality with your input catalog, just talk to Erin. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp /work/05350/ecooper/stampede2/3dhst/3dhst_input.cat ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 /work/05350/ecooper/stampede2/HETDEX_API/get_spec.py --multiprocess True -i '3dhst_input.cat' -o '3dhst' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Batch Job production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help those with lists that need to access more than ~200 shots here is an example of how you can break down extractions into several slurm tasks giving each shot a separate process. First use the `get_shots_of_interest.py` function to retrieve a text list of SHOTIDs stored in the file `shotlist` in the working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 /work/05350/ecooper/stampede2/HETDEX_API/get_shots_of_interest.py -i '3dhst_input.cat'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then port each python get_spec.py command into a shell script for batch processing using the handy unix program `awk`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! awk '{print \"python3 /work/05350/ecooper/stampede2/HETDEX_API/get_spec.py -i 3dhst_input.cat -s\", $1, \"-o\", $1}' shotlist > run_shot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then use `jobsplitter_stampede2` to split the processes into files of 192 jobs each. This will distribute the processes across the cluster giving each node 48 processes to run. Each slurm job in this case will use 4 Nodes. These are all options that you can give the slurm launcher, but we have found its better to limit Nodes =< 4 and time to less than 30 min if you don't want to wait forever to get in the queue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! /work/05350/ecooper/stampede2/HETDEX_API/hdr1_bash_scripts/jobsplitter_stampede2 run_shot 128 1 '00:30:00' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the batch jobs, go to a login node in the working directory (on /work) that contains your input catalog and enter:\n",
    "    \n",
    "`sbatch run_shot_1.batch`\n",
    "\n",
    "`sbatch run_shot_2.batch`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you need to combine all the files together to make for easier analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 /work/05350/ecooper/stampede2/HETDEX_API/get_spec.py --merge True --outfile '3dhst_master'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading in the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although it may not be immediately familiar to us astronomers who love fits files, the easiest way to take a bunch of extracted spectra and do something with it is to store it an intermediate binary file which will preserve the nested dictionary format of the output. It means less files produced and less storage space used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sources = pickle.load( open( 'cosmos_LAE.pkl', 'rb'))\n",
    "#Sources = pickle.load( open( 'mptest.pkl', 'rb'))\n",
    "#Sources = pickle.load( open( '3dhst.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wave = 2.0 * np.arange(1036) + 3470.\n",
    "count = 0\n",
    "for id in Sources.keys():\n",
    "    for shotid in Sources[id].keys():\n",
    "        plt.plot(wave, Sources[id][shotid][0])\n",
    "        plt.title('ID: '+str(id)+'   SHOTID: ' + str(shotid))   \n",
    "        count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zoom on emission line\n",
    "wave_obj = 3709.74\n",
    "for id in Sources.keys():\n",
    "    for shotid in Sources[id].keys():\n",
    "        plt.xlim([ wave_obj - 50, wave_obj + 50])\n",
    "        plt.plot(wave, Sources[id][shotid][0])\n",
    "        plt.title('ID: '+str(id)+'  SHOTID: ' + str(shotid))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Line Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I show an example of how to fit a line in a region where you think a line might be. There are other tools you can use to find lines, please read up on specutils. Much more testing needs to be done here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "from astropy.modeling import models\n",
    "from specutils import Spectrum1D, SpectralRegion\n",
    "from specutils.analysis import equivalent_width\n",
    "from specutils.fitting import estimate_line_parameters\n",
    "from specutils.manipulation import extract_region\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_fit(obj, wave_obj, datevobs):\n",
    "    spectrum = Spectrum1D( flux = Sources[obj][datevobs][0]*u.erg * u.cm**-2 / u.s / u.AA, \n",
    "                        spectral_axis= (2.0 * np.arange(1036) + 3470.)*u.AA)\n",
    "    \n",
    "    sub_region = SpectralRegion((wave_obj-10)*u.AA, (wave_obj+10)*u.AA)\n",
    "    \n",
    "    sub_spectrum = extract_region(spectrum, sub_region)\n",
    "\n",
    "    line_param =  estimate_line_parameters(sub_spectrum, models.Gaussian1D())\n",
    "    print(line_param.amplitude.value, line_param.fwhm.value)\n",
    "#    if (line_param.amplitude.value > 0) & (line_param.fwhm.value <10):\n",
    "    plt.figure()\n",
    "    plt.plot(spectrum.spectral_axis, spectrum.flux)\n",
    "    plt.title('ID: '+ str(obj) + '   ObsID: '+ str(datevobs))\n",
    "    plt.xlim([ wave_obj - 50, wave_obj + 50])\n",
    "    plt.xlabel('Spectral Axis ({})'.format(spectrum.spectral_axis.unit))\n",
    "    plt.ylabel('Flux Axis({})'.format(spectrum.flux.unit))\n",
    "    plt.savefig('spec_' + str(obj) + '_' + str(datevobs) + '.png')\n",
    "    #plt.ylabel('1e-17 ergs/s/cm^2/A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wave_obj = 5394.\n",
    "wave_obj = 3709.74"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for obj in Sources.keys():\n",
    "    for datevobs in Sources[obj].keys():\n",
    "        count += 1\n",
    "        line_fit( obj, wave_obj, datevobs) # note could use multiprocessing Pool to really speed this up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximate magnitude from spectrum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how a took the pickle output from fitting 9K SDSS stars and calculated the gmagnitude from the 1D spectrum. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speclite\n",
    "import speclite.filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an example of how extractions were performed, you can see the directory:\n",
    "    \n",
    "    /work/05350/ecooper/stampede2/sdss_comps/test_4nodes\n",
    "    \n",
    "The main process list is here:\n",
    "\n",
    "    /work/05350/ecooper/stampede2/sdss_comps/test_4nodes/run_shot\n",
    "    \n",
    "Which then was broken into several slurm jobs that I submitted one at a time (I found sending too, slowed my time to get in the queue as well as processing time greatly:\n",
    "\n",
    "    /work/05350/ecooper/stampede2/sdss_comps/test_4nodes/run_shot_X.run\n",
    "\n",
    "Each job is run with:\n",
    "\n",
    "    sbatch run_shot_X.slurm\n",
    "    \n",
    "After all the jobs are complete, I combined all the pickle files together using the merge option within this directory:\n",
    "\n",
    "    python3 /work/05350/ecooper/stampede2/HETDEX_API/get_spec.py --merge True --outfile sdss_master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you load the pickle and the input catalog as that has the SDSS g-mags. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdss_table = Table.read('/work/05350/ecooper/stampede2/sdss_comps/test_4nodes/sdss_star.cat', format='ascii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sources = pickle.load( open( '/work/05350/ecooper/stampede2/sdss_comps/test_4nodes/sdss_master.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_table = Table(names=['ID','shotid','gmag_hetdex'], dtype=['<i8','<i8', '<f'])\n",
    "\n",
    "filt = speclite.filters.load_filters('sdss2010-g')\n",
    "wave_rect = 2.0 * np.arange(1036) + 3470.\n",
    "\n",
    "for ID in Sources.keys():\n",
    "    for shotid in Sources[ID].keys():\n",
    "        spec = Sources[ID][shotid][0]\n",
    "\n",
    "        sel = np.isfinite(spec)\n",
    "        \n",
    "        if np.sum(sel) > 0:\n",
    "            flux, wlen = filt.pad_spectrum( np.array( (1.e-17) * spec[sel]),\n",
    "                                            np.array(wave_rect[sel]))\n",
    "        else:\n",
    "            flux, wlen = filt.pad_spectrum( np.array( (1.e-17) * spec),\n",
    "                                            np.array(wave_rect))\n",
    "#        plt.plot(wlen, flux)\n",
    "\n",
    "        gmag = filt.get_ab_magnitudes(flux, wlen)[0][0] - filt.get_ab_maggies(flux, wlen)[0][0]\n",
    "#        spec_err = np.sqrt( np.sum( ( Sources[ID][shotid[1]])**2 ))\n",
    "        out_table.add_row([ID, shotid, gmag])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example comparison plot using "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_table = join(out_table, sdss_table, keys='ID')\n",
    "sel = (combined_table['g'] > 0) * (combined_table['gmag_hetdex'] > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 18})\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(np.arange(13,28), np.arange(13,28), color='r')\n",
    "sel = (combined_table['g'] > 0) * (combined_table['gmag_hetdex'] > 0)\n",
    "n = np.sum(sel)\n",
    "plt.hist2d(combined_table['g'][sel], combined_table['gmag_hetdex'][sel], bins=75)\n",
    "plt.plot(np.arange(13,28), np.arange(13,28), color='r')\n",
    "plt.xlabel('SDSS - g')\n",
    "plt.ylabel('HETDEX 1D summed in sdss-g ')\n",
    "plt.xlim(14.3, 26)\n",
    "plt.ylim(14.3, 26)\n",
    "plt.colorbar()\n",
    "plt.title('n_stars = '+ str(n))\n",
    "#plt.savefig('sdss_gmag_comps.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
